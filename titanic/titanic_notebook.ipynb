{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['titanic_notebook.ipynb', 'test.csv', 'train.csv', '.ipynb_checkpoints', 'gender_submission.csv']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#actual test data\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "#training set\n",
    "train_df = pd.read_csv('train.csv')\n",
    "\n",
    "#sample submission\n",
    "gender_df = pd.read_csv('gender_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[OrderedDict([('PassengerId', '1'),\n",
       "              ('Survived', '0'),\n",
       "              ('Pclass', '3'),\n",
       "              ('Name', 'Braund, Mr. Owen Harris'),\n",
       "              ('Sex', 'male'),\n",
       "              ('Age', '22'),\n",
       "              ('SibSp', '1'),\n",
       "              ('Parch', '0'),\n",
       "              ('Ticket', 'A/5 21171'),\n",
       "              ('Fare', '7.25'),\n",
       "              ('Cabin', ''),\n",
       "              ('Embarked', 'S')])]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "with open('train.csv', 'rt') as f:\n",
    "    data = list(csv.DictReader(f))\n",
    "data[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "891 items total, 38.4% true\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "_all_xs = [{k: v for k, v in row.items() if k != 'Survived'} for row in data]\n",
    "_all_ys = np.array([int(row['Survived']) for row in data])\n",
    "\n",
    "all_xs, all_ys = shuffle(_all_xs, _all_ys, random_state=0)\n",
    "train_xs, valid_xs, train_ys, valid_ys = train_test_split(\n",
    "    all_xs, all_ys, test_size=0.25, random_state=0)\n",
    "print('{} items total, {:.1%} true'.format(len(all_xs), np.mean(all_ys)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#minimal preprocessing: convert obviously contiuous Age and Fare variables to floats, \n",
    "# and SibSp, Parch to integers. Missing Age values are removed.\n",
    "\n",
    "for x in all_xs:\n",
    "    if x['Age']:\n",
    "        x['Age'] = float(x['Age'])\n",
    "    else:\n",
    "        x.pop('Age')\n",
    "    x['Fare'] = float(x['Fare'])\n",
    "    x['SibSp'] = int(x['SibSp'])\n",
    "    x['Parch'] = int(x['Parch'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'PassengerId': '496', 'Pclass': '3', 'Name': 'Yousseff, Mr. Gerious', 'Sex': 'male', 'SibSp': 0, 'Parch': 0, 'Ticket': '2627', 'Fare': 14.4583, 'Cabin': '', 'Embarked': 'C'}, {'PassengerId': '649', 'Pclass': '3', 'Name': 'Willey, Mr. Edward', 'Sex': 'male', 'SibSp': 0, 'Parch': 0, 'Ticket': 'S.O./P.P. 751', 'Fare': 7.55, 'Cabin': '', 'Embarked': 'S'}, {'PassengerId': '279', 'Pclass': '3', 'Name': 'Rice, Master. Eric', 'Sex': 'male', 'Age': 7.0, 'SibSp': 4, 'Parch': 1, 'Ticket': '382652', 'Fare': 29.125, 'Cabin': '', 'Embarked': 'Q'}, {'PassengerId': '32', 'Pclass': '1', 'Name': 'Spencer, Mrs. William Augustus (Marie Eugenie)', 'Sex': 'female', 'SibSp': 1, 'Parch': 0, 'Ticket': 'PC 17569', 'Fare': 146.5208, 'Cabin': 'B78', 'Embarked': 'C'}, {'PassengerId': '256', 'Pclass': '3', 'Name': 'Touma, Mrs. Darwis (Hanne Youssef Razi)', 'Sex': 'female', 'Age': 29.0, 'SibSp': 0, 'Parch': 2, 'Ticket': '2650', 'Fare': 15.2458, 'Cabin': '', 'Embarked': 'C'}, {'PassengerId': '299', 'Pclass': '1', 'Name': 'Saalfeld, Mr. Adolphe', 'Sex': 'male', 'SibSp': 0, 'Parch': 0, 'Ticket': '19988', 'Fare': 30.5, 'Cabin': 'C106', 'Embarked': 'S'}, {'PassengerId': '610', 'Pclass': '1', 'Name': 'Shutes, Miss. Elizabeth W', 'Sex': 'female', 'Age': 40.0, 'SibSp': 0, 'Parch': 0, 'Ticket': 'PC 17582', 'Fare': 153.4625, 'Cabin': 'C125', 'Embarked': 'S'}, {'PassengerId': '319', 'Pclass': '1', 'Name': 'Wick, Miss. Mary Natalie', 'Sex': 'female', 'Age': 31.0, 'SibSp': 0, 'Parch': 2, 'Ticket': '36928', 'Fare': 164.8667, 'Cabin': 'C7', 'Embarked': 'S'}, {'PassengerId': '485', 'Pclass': '1', 'Name': 'Bishop, Mr. Dickinson H', 'Sex': 'male', 'Age': 25.0, 'SibSp': 1, 'Parch': 0, 'Ticket': '11967', 'Fare': 91.0792, 'Cabin': 'B49', 'Embarked': 'C'}, {'PassengerId': '368', 'Pclass': '3', 'Name': 'Moussa, Mrs. (Mantoura Boulos)', 'Sex': 'female', 'SibSp': 0, 'Parch': 0, 'Ticket': '2626', 'Fare': 7.2292, 'Cabin': '', 'Embarked': 'C'}, {'PassengerId': '705', 'Pclass': '3', 'Name': 'Hansen, Mr. Henrik Juul', 'Sex': 'male', 'Age': 26.0, 'SibSp': 1, 'Parch': 0, 'Ticket': '350025', 'Fare': 7.8542, 'Cabin': '', 'Embarked': 'S'}, {'PassengerId': '347', 'Pclass': '2', 'Name': 'Smith, Miss. Marion Elsie', 'Sex': 'female', 'Age': 40.0, 'SibSp': 0, 'Parch': 0, 'Ticket': '31418', 'Fare': 13.0, 'Cabin': '', 'Embarked': 'S'}]\n"
     ]
    }
   ],
   "source": [
    "print(all_xs[:12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## xgboost classifier   \n",
    "very simple classifier with xbgoost.XGBClassifier and sklearn.feature_extraction.DictVectorizer,   \n",
    "and check its accuracy with 10-fold cross-validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:22:24] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:22:24] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:22:24] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:22:24] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:22:24] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:22:25] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:22:25] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:22:25] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:22:25] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:22:26] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy: 0.820 Â± 0.064\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Pipeline.fit does not accept the eval_metric parameter. You can pass parameters to specific steps of your pipeline using the stepname__parameter format, e.g. `Pipeline.fit(X, y, logisticregression__sample_weight=sample_weight)`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-ab19b07d8e39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0m_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_xs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_ys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_metric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'logloss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# so that parts of the original pipeline are fitted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-23-ab19b07d8e39>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(_clf)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_clf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_xs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_ys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy: {:.3f} Â± {:.3f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0m_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_xs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_ys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_metric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'logloss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# so that parts of the original pipeline are fitted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/py37/lib/python3.7/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mThis\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \"\"\"\n\u001b[0;32m--> 329\u001b[0;31m         \u001b[0mfit_params_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_fit_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m         \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         with _print_elapsed_time('Pipeline',\n",
      "\u001b[0;32m/opt/anaconda3/envs/py37/lib/python3.7/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_check_fit_params\u001b[0;34m(self, **fit_params)\u001b[0m\n\u001b[1;32m    246\u001b[0m                     \u001b[0;34m\"pipeline using the stepname__parameter format, e.g. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m                     \u001b[0;34m\"`Pipeline.fit(X, y, logisticregression__sample_weight\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m                     \"=sample_weight)`.\".format(pname))\n\u001b[0m\u001b[1;32m    249\u001b[0m             \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'__'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m             \u001b[0mfit_params_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Pipeline.fit does not accept the eval_metric parameter. You can pass parameters to specific steps of your pipeline using the stepname__parameter format, e.g. `Pipeline.fit(X, y, logisticregression__sample_weight=sample_weight)`."
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "clf = XGBClassifier(use_label_encoder=False)\n",
    "vec = DictVectorizer()\n",
    "pipeline = make_pipeline(vec, clf)\n",
    "\n",
    "def evaluate(_clf):\n",
    "    scores = cross_val_score(_clf, all_xs, all_ys, scoring='accuracy', cv=10)\n",
    "    print('Accuracy: {:.3f} Â± {:.3f}'.format(np.mean(scores), 2 * np.std(scores)))\n",
    "    _clf.fit(train_xs, train_ys, eval_metric='logloss', verbose=True)  # so that parts of the original pipeline are fitted\n",
    "\n",
    "evaluate(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "# read in data\n",
    "dtrain = xgb.DMatrix('demo/data/agaricus.txt.train')\n",
    "dtest = xgb.DMatrix('demo/data/agaricus.txt.test')\n",
    "# specify parameters via map\n",
    "param = {'max_depth':2, 'eta':1, 'objective':'binary:logistic' }\n",
    "num_round = 2\n",
    "bst = xgb.train(param, dtrain, num_round)\n",
    "# make prediction\n",
    "preds = bst.predict(dtest)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
